# -*- coding: utf-8 -*-
"""open ai whisper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KGYk02qMcD20JxoZRZxquXKDsm7ttnr4
"""

!pip install git+https://github.com/openai/whisper.git -q

!nvidia-smi -L

"""Whisper transcribes audio speech to text. To see this in action, we'll need some audio to operate on. We'll extract some audio from a YouTube video using the pytube Python package. let's install it with pip."""

!pip install pytube -q

"""Now that we have installed the dependencies, let's import whisper and the YouTube class from pytube."""

import whisper
from pytube import YouTube

model = whisper.load_model('base')



"""We'll now instantiate a "YouTube" object by passing in the video URL. This will allow us to retrieve metadata and stream info about the specified YouTube URL."""

youtube_video_url = "https://www.youtube.com/watch?v=5JqGeP240Tc"
youtube_video = YouTube(youtube_video_url)

youtube_video.title

dir(youtube_video)

youtube_video.streams

for stream in youtube_video.streams:
  print(stream)

streams= youtube_video.streams.filter(only_audio= True)

streams

stream.download(filename='ted_talk.mp4')

!ffmpeg -ss 20 -i ted_talk.mp4 -t 9000 ted_talk_trimmed.mp4

import datetime

# save a timestamp before transcription
t1 = datetime.datetime.now()
print(f"started at {t1}")

# do the transcription
output = model.transcribe("ted_talk_trimmed.mp4")

# show time elapsed after transcription is complete.
t2 = datetime.datetime.now()
print(f"ended at {t2}")
print(f"time elapsed: {t2 - t1}")

output

output['text']

for segment in output['segments']:
  print(segment)
  second = int(segment['start'])
  second = second - (second % 5)
  print(second)

